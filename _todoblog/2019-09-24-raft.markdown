---
layout: post
title: raft
date: 2019-09-24 18:06
header-img: "img/head.jpg"
categories: jekyll update
tags:
	- Raft
typora-root-url: ../../yummyliu.github.io
---
* TOC
{:toc}
# raft论文总结

> 我这里对一组需要达成一致的机器称为共识组，共识组内使用Paxos或者Raft等共识算法保证一致性。

相对于Paxos算法的难以理解，Raft是一个相对容易理解的共识算法；其将算法分为多个模块（LeaderElection/LogReplication/Safety）,并减少了状态空间的复杂度，这样减少了整体的不确定程度。同时Raft也有自己的特点：

+ StrongLeader：logentries只能从leader向follower流动；
+ LeaderElection：在共识组中，按照随机时间触发Leader选举，这在原来的hearbeat中只添加了很少的额外工作。
+ MembershipChange：基于joint consensus过程，向共识组中添加新的成员；在过渡过程中，保证共识组仍能正常工作。

## 可复制状态机

> **拜占庭将军问题** : 在存在消息丢失的不可靠信道上试图通过消息传递的方式达到一致性是不可能的。因此对一致性的研究一般假设信道是可靠的，或不存在本问题。

对于系统设计中的单点问题，如果是一个无状态的服务，那么可以无脑的分成多个节点，前段加一个负载均衡。而要保证一个有状态的服务的可用性，一般就是通过复制做一个备点，比如数据库的主备。

生产中可用的共识算法，有如下特性：

+ 在非拜占庭情况下，包括网络延迟、网络分割、丢包、重复发送、信息乱序，保证信息的安全（不会返回一个错误的结果）。

+ 在多数节点可用时，保证系统的可用性。
+ 不依赖时间来保证日志的一致性，因为如果时钟有错，或者信息延迟很长时间，那么会造成系统长时间不可用。
+ 多数节点完成了一个指令后，那么就应该对client进行响应；不应因为少数节点，影响整体性能。

## Paxos的弊端与Raft对应的优势

Paxos首先定义了一个BasicPaxos，对单个logentry进行共识决策；然后结合多个BasicPaxos，实现一个MultiPaxos，对多个logentry进行决策；其可用性和正确性都被证实过，但是有两个显著的弊端：

1. 难以理解
2. 没有leader的对等架构，实现起来比较复杂，导致最终实现区别比较大。

针对Paxos的弊端，Raft设计出来了；主要解决的问题就是更容易被人理解；

首先将需要解决的问题进行分解，分解为：leaderelection、logreplication、safety、membershipchange。

然后，减少需要维护的状态，进行减少系统可能的不确定性；比如，日志中不能有空洞。

## Raft

Raft首先在共识组中选出一个leader，然后由leader全权处理LOG的复制；leader首先从client处得到logentry，然后该logentry请求复制转发到其他server中；之后，leader再通知其他server进行apply。这样由leader来统一处理，简化了日志的复制管理。由此，Raft算法可分为以下几个部分：

+ Leader Election：当leader失效后，需要选举处一个新的leader。
+ Log Replication：leader收到新log entry时，需要及时复制给其他server，并确保其他log与自己本地log达成一致。
+ Safety：当某个server将某个log index处的log进行了apply后，那么在整个集群内，该log index处都应该apply同一条log。

在Raft共识组中，任何时间只有三种角色：Leader/Follower/Candidate。大部分时间为一个leader和多个follower；candidate用在新leader选举中。follower是一个被动节点，只是响应leader和candidate的请求；如果client连接到了follower上，也是会被转发到leader上处理。共识组中的信息交互通过rpc，主要有三种：

+ RequestVote：Candidate发起投票请求；
+ AppendEntries：Leader复制log entry，以及发送hearbeat。
+ TransferSnapshot：

### Leader选举

![image-20191029151530653](/image/1029-raft-election.png)

Raft以任期（term）的方式划分时间，每个任期开始的时候，进行一次选举；在每次选举中，可能有一个或多个candidate；term是一个单调递增的序列，在每个server中，存储了一个current term，并且会和其他server进行交换，如果发现自己的term比别人的小，那么就更新为大的term（如果自己的角色不是follower，那么就变成follower）。

**RequestVote**：距离上次leader发来信息超过election timeout，超时后follower变成candidate，递增自己的term，并发调用RequestVote，向其他server请求投票；收到RequestVote请求的server，在任一任期内，只能给一个candidate投票。

如果candidate收到一个leader的AppendEntries请求，如果该请求的term值与自己的term值大，那么自己在该term内成为follower，否则拒绝该请求，继续选举。

![image-20191029152732563](/image/1029-raft-term.png)

另外，有可能在某个任期内，很多follower同时变成candidate，那么可能由于分票，导致选举失败；Raft采用随机设置election timeout的方式（每个follower的超时时间的随机的[150ms, 300ms]），减少多个follower同时选举的情况，来规避这个问题。

> 这里除了采用随机超时的方式解决外，还考虑了给各个server一个优先级排序；这种情况存在一个问题：如果高优先级节点也挂了，那么低优先级需要等待一个超时后，才能自己变成candidate。

### Log复制

Log中每个Entry包含一个logindex和一个termid，用来保证每个logindex的内容都是唯一的；并且有如下保证：

+ 两个不同log中的logentry有相同的logindex和term，那么其中的内容是相同的；由于leader在每个index处，只创建一个logentry，并且follower不会更改logentry的位置，那么确保该项成立。
+ 两个不同log中的logentry有相同的logindex和term，那么之前的logentry都是相同的；leader在AppendEntries请求中，会带上new logentry前一个logentry的logindex和term，如果follower在本地日志中没有找到该项logentry，就拒绝这个new logentry。因此，知道follower响应了AppendEntries请求， 那么leader就知道follower已经同步到的该位置。

![image-20191029174944957](/image/1029-raft-logreplication.png)

1. client请求leader 写也一个新的logentry；
2. leader将变更记在本地log中
3. leader通过AppendEntries将变更发送给follower；
4. follower收到logentry将其写到本地log中，该logentry状态为uncommited；
5. leader收到大多数follower的ack，那么该变更标记为commited；对于没有收到ack的follower继续重试。
6. leader将commited的logentry应用到state machine中，leader给client返回成功；在leader的AppendEntries请求中会包含最大的commited的logindex，这样follower都知道当前可提交的logentry。

当新leader初始化时，对于每个follower初始化一个**nextIndex**（等于leader当前的index），然后通过多次AppendEntries，将nextIndex向前推进到正确的位置中。后续leader发送new entry，follower当收到的new logentry和follower本地的相同index处的logentry不同时，用new logentry覆盖（这里可能会覆盖已经commit的日志，那么存在问题，下一章针对该问题进行讨论如何避免，其实这就是保证Leader Completeness）。

![image-20191029183030601](/image/1029-raft-leader-init-nextindex.png)

综上，发现只要有大多数节点活着，大部分情况下，一次rpc就能提交。

>  脑裂
>
> 此时，由于大多数的前提，那么只有一个部分存在leader，这时发送到这个leader的才能顺利commit；而另一个网络中收到的请求，由于没有大多数投票，因此不能commit。
>
> 当网络恢复时，另一个网路收到的leader的请求，并回滚自己的未提交的日志。

### Safety

Raft确保共识组运行中，以下五个特性始终满足：

1. Election Safety：每个term中最多只有一个leader；在leader 选举中保证了。
2. Leader Append-Only：Leader对于自己的日志只会追加，不修改和删除；在Log复制中保证了。
3. Log Matching：如果两个日志包含一个logindex和term都相同的entry，那么该Entry之前的日志都相同；在Log复制中保证了。
4. Leader Completeness：如果一个log entry在某个term中已经提交了，那么之后的term换了leader，该commited的entry仍然在日志中。
5. State Machine Satety：如果状态机已经应用了某条日志，那么在该logindex处，不能apply其他entry。

在Raft的vote过程中，只有包含所有commited的entry的candidate才会赢得选举；candidate在发起RequestVote请求中，会包含自己的log信息；follower收到请求后，会判断自己的log是否比candidate的log更新，follower只会响应比自己新的candidate的请求；那么，只有上一个term的多数派中的其中一个follower，才有可能成为新的leader，因此，新的leader永远包含所有commited的数据，而之后只是接着向后写，不会更改自己的log。

> 如何比较日志更新？
>
> ```python
> if my_termid > can_termid:
>   return "I am later"
> elif my_termid == can_termid and my_logindex > can_logindex:
>   return "I am later"
> else:
>   return "he is laster"
> ```

一个新的leader中，可能有一些之前没有标记commited的entry；新leader仍然按照旧的termid，将这些entry复制到follower中，继续之前任期的提交操作（在一些其他共识算法中，这些entry会换成自己的termid，重新复制）。问题确认？？

由于只有包含最新的logentry的candidate才会赢得选举，所以新的leader中，一定有之前leader最新提交的entry；因此，一定能保证Leader Completeness；进而可以保证State Machine Safety。

对于Follower和Candidate的失效，leader会不断的重试；即使follower在完成了rpc请求后，挂了，由于Raft的Rpc请求是幂等的，重启后，再次重复也没事。

> 确定election timeout时间 需要保证 `broadcastTime << electionTimeout << MTBF`
>
> + broadcastTime是一个server并发给其他server发送RPC，并得到响应的时间
> + electionTimeout是何时发起新的选举
> + MTBF是单个server失效的平均间隔时间

# MIT-Lab2

## 2A:LeaderElection&Heartbeat

## 2B:LogReplication

## 2C:Persistance&Recovery