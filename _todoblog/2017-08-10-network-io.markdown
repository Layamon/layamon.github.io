---
layout: post
title: IO in Network
date: 2017-08-10 18:03
header-img: "img/head.jpg"
categories: 
    - Linux
---



# 多线程，基于回掉的异步IO，协程

+ one-thread-per-client: 线程切换代价高--CFS调度方式：O(log(m)),m约等于 活跃的上下文数，大概是和客户端数量相当
+ IOCP(Input/Output Completion Port): 支持多个同时发生的异步IO操作的应用程序编程接口。(主要是Window系统下，linux下相应为epoll，但是epoll不负责读写，只是通知)
+ Callback: 上层代码将一个函数对象or指针,传到底层库中，一般注册到某个事件上，当事件发生就回调
+ Sync/Async: 在C/S模型中，主要针对C端, Sync就是 (提交请求->等待服务器处理->处理完毕返回 ), Async就是 提交请求立即返回，继续做其他的事,但是注册一个回调，有消息那么调用回调，
+ Block/Unblock: 在C/S模型中，主要针对S端，Block和同步的区别：Block当前线程会被挂起，而同步调用当前线程还是激活的;
+ COW(copy on write): 在Linux程序中，fork()会产生一个和父进程完全相同的子进程，但子进程在此后多会exec系统调用，出于效率考虑，Linux中引入了“写时复制“技术，也就是只有进程空间的各段的内容要发生变化时，才会将父进程的内容复制一份给子进程。

##  one-thread/process-per-client，阻塞模式

线程切换代价log(m), 每次IO阻塞(这里假设thread模型的io都是阻塞的)，都会发生开销。而且决定活跃线程数的是用户，
这不是我们可控制的。更糟糕的是，当性能下降，响应速度下降时。同样的用户数下，活跃上下文会上升(因为响应变慢了)。这会进一步拉低性能。

## 非阻塞模式

既然是非阻塞，那么一定会有就绪通知的问题，如果没有就绪通知，我们只能在非阻塞的fd上轮询，直到遇到一个就绪的fd；
两种方案：就绪事件通知和异步IO, 前者只是同时数据就绪了，后者负责数据读写，完成后通知

**Linux下的主要方案就是就绪通知，select/poll/epoll**, 操作系统级别的异步IO应该linux没有（目前了解的）,window下实现了一个接口

非阻塞的方式写代码有点难写，协程将异步非阻塞的技术中，事件回调进行了包装, 当某一个协程中进行io操作的时候，保存当前的堆栈,
换别的协程执行，比起系统线程来说，开销小，缺点就在于如果其中一个协程有密集计算，其他的协程就不运行了。

# 事件通知机制上的几种编程模型

## 协程

### lico

腾讯开源的c++的协程库

基于操作系统提供的切换上下文的技术,makecontext/swapcontext

1. 包装read/write, 在发生read的时候检查返回，如果是EAGAIN， 那么将当前协程标记为阻塞在对应fd上，然后执行调度函数。
2. 调度函数需要执行epoll(或者从上次的返回结果缓存中读数据，减少内核陷入次数), 从中读取一个就绪的fd。
    如果没有，上下文应当被阻塞到至少有一个fd就绪
3. 查找这个fd对应的协程上下文对象，并调度过去
4. 当协程被调度到时，重新尝试读取

### CPS模型（Continuation-Passing Style）

CPS 可以总结为一个很重要的思想: 
_“我不用等执行结果，我先假设结果已经有了，然后描述一下如何利用这个结果，至于调用的时机，由结果提供方负责管理”。_

我们抽象一下，CPS 的含义是不直接等待异步数据返回，而是传入一个回调函数来处理未来的数据。换句话讲:

回调事件是一个普通事件，内部可能还会发起一个异步事件

好处在于，**通过事件的嵌套形成了一套递归模型，理论上能够解决任意多层的嵌套。**
当然缺点也是显而易见的，语义上的嵌套最终导致了代码上的嵌套，影响了可读性和可维护性。

### IO在什么时间发生？

核心在于，整个回调模型是基于多路复用的，还是基于异步IO的

原则上两者都可以。你可以监听fd就绪，也可以监听IO完成。当然，即使监听IO完成，也不代表使用了内核态异步接口。很可能只是用epoll封装的而已。

