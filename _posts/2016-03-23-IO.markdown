---
layout: post
title: IO ,linux
date: 2016-03-23 16:11
categories: jekyll update
---

#### 本地IO

> 在linux操作系统上，怎样保证对文件的更新内容成功持久化到硬盘？

##### fsync

一般情况下，对硬盘（或者其他持久存储设备）文件的write操作，更新的只是内存中的页缓存（page cache），   
而脏页面不会立即更新到硬盘中，而是由操作系统统一调度，如由专门的flusher内核线程在满足一定条件时（如一定时间间隔、内存中的脏页达到一定比例）内将脏页面同步到硬盘上（放入设备的IO请求队列）。  
因为write调用不会等到硬盘IO完成之后才返回，因此如果OS在write调用之后、硬盘同步之前崩溃，则数据可能丢失。   虽然这样的时间窗口很小，但是对于需要保证事务的持久化（durability）和一致性（consistency）的数据库程序来说，write()所提供的“松散的异步语义”是不够的，  
通常需要OS提供的同步IO（synchronized-IO）原语来保证：

``` c
#include <unistd.h>
int fsync(int fd);
```

fsync的功能是确保文件fd所有已修改的内容已经正确同步到硬盘上，  
该调用会阻塞等待直到设备报告IO完成。

##### msync

如果采用内存映射文件的方式进行文件IO,  
使用mmap，将文件的page cache直接映射到进程的地址空间，通过写内存的方式修改文件，  
也有类似的系统调用来确保修改的内容完全同步到硬盘之上：

##### fdatasync

除了同步文件的修改内容（脏页），fsync还会同步文件的描述信息（metadata，包括size、访问时间st_atime & st_mtime等等），  
因为文件的数据和metadata通常存在硬盘的不同地方，因此fsync至少需要两次IO写操作  
同步的metadata中，关键的就是filesize，  
如果filesize大小不变，那么就可以使用fdatasync优化同步IO

``` c
#include <unistd.h>
int fdatasync(int fd);
```

例子，Berkeley DB处理数据库日志文件

```
1.每个log文件固定为10MB大小，从1开始编号，名称格式为“log.%010d"
2.每次log文件创建时，先写文件的最后1个page，将log文件扩展为10MB大小
3.向log文件中追加记录时，由于文件的尺寸不发生变化，使用fdatasync可以大大优化写log的效率
4.如果一个log文件写满了，则新建一个log文件，也只有一次同步metadata的开销
```

#### 网络IO

##### 流机制

linux 提供的构造内核设备驱动程序和网络协议包的一种通用方法  
用户进程->流首（系统调用界面）(->处理模块 使用ioctl命令可压入处理模块)->设备驱动程序  

##### 综述

1. n个进程阻塞读;
    这种情况下，使用线程比使用进程简单，终止的时候，父子进程之间需要通信告知,但是使用线程就会带来线程同步的损失
    
2. 将n个描述符，设成非阻塞的方式（fcntl）,轮询之;
   这样大部分时候是在浪费了CPU时间,多任务系统中很不可取

3. 异步io,(ioctl)
    不再自己主动去问了，当描述符准备好了，内核再通知它; 可是进程收到这个信号后，如果等的不止一个描述符，这个时候，就不知道是哪个准备好了，所以需要将这个描述符，设成非阻塞的轮询一遍

4. I/O多路转接:调用一个函数,告诉内核我们关心的描述符的列表，以及我们关心其IO的那种状态（读/写/异常）,届时函数返回告诉我们

+ 使用轮询的方式的系统调用:
    select/pselect
    准备好的描述符 select(最大描述符+1,read状态组，write状态组，exception状态组，可等待时间,(pselect:信号屏蔽字))
    poll
    pollfd{
        int fd;
        short events;//关心的事件
        short reevents;//最后发生的事件
    }
    int poll(fdarray[],nfds(fd数)，timeout)
    同样是告诉系统相关信息，只是形式不同，这个不用对三种状态，各维护一个列表

+ 采用中断信号驱动的方式:

    *模式：reactor/proactor*
    个人理解：户进程接收的是什么信号，dispatcher 是位于操作系统内核中 还是个人用户进程实现的
    IO多路转接中，涉及到三个角色，hander(用户进程的处理模块),dispatcher(调度模块，接收数据),demultiplexer(类似于计组里的多路信号选择器)
    reactor:dispatcher是用户进程中的，demultiplexer通知用户进程的是准备IO的信号
    proactor: 首先使用这个模式需要 操作系统层面支持异步io，dispatcher就是操作系统里的一个线程whatever，demultiplexer通知用户进程的是IO完成的信号
    既然这样的区别，那么我们在reactor的基础上，自己实现一个dispatcher,就可以自己实现一个proactive的io接口

    epoll (linux)  reactor
    kqueue (BSD ~mac) reactor
    iocp (window) proactor

    以linux2.6.27之后 epoll_create1(int flag) flag:0 和epoll_create相同，不过之前设定size，现在内核可以动态分配，所以现在一般使用 EPOLL_CLOEXEC,因为create 时候也会占用一个系统fd，应及时close（） 或者像这样 
    epoll_ctl(int eptd(create 返回的epoll 实例),
            int op(添加 删除 修改操作),
            int fd(描述符),
            struct epoll_event*(关心的事件)) 
    epoll_wait(int epfd(epoll实例),struct epoll_event* events,int maxevents,int timeout)

    from linux 手册: **The epoll interface, when used with the EPOLLET flag ( Edge Triggered ) should use non-blocking file descriptors to avoid having a blocking read or write starve the task that is handling multiple file descriptors**


**linux中异步i/o只对，streams设备和streams管道起作用，select poll对任何操作符都起作用**

5. readv/writev
    一次函数调用中读写多个非连续的缓冲区。
    同样是实现这一个操作，我们可以调用多次read、write；或者申请一个足够大的用户缓冲区,将这些复制进去，然后调用一次write
前者多次系统调用，开销必然大；
后者多了一步复制的开销,这一步复制的开销需要和readv/writev实现必然比read write复杂的开销做权衡

6. 存储映射
    mmap,将磁盘文件和内存缓冲区对应上，读写缓冲区就是读写文件,映射区域系统虚存的页长度相关，会做padding
